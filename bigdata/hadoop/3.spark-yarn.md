* https://spark.apache.org/docs/latest/running-on-yarn.html
* https://spark.apachecn.org/#/docs/17
* https://spark.apache.org/docs/latest/configuration.html


# 部署spark
>在hadoop yarn节点上
* 下载：https://spark.apache.org/downloads.html
```
wget https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz
tar -xzf spark-3.2.1-bin-hadoop3.2.tgz -C /opt
ln -s spark-3.2.1-bin-hadoop3.2 spark
cd spark
```
```
chown -R hadoop.hadoop /opt/spark/
su - hadoop
```

# 配置spark yarn
>与 Spark standalone 和 Mesos 不同的是，在这两种模式中，master 的地址在 --master 参数中指定。在 YARN 模式下，ResourceManager 的地址从 Hadoop 配置中选取
* cp conf/spark-env.sh.template conf/spark-env.sh
```
# 确保 HADOOP_CONF_DIR 或者 YARN_CONF_DIR 指向包含 Hadoop 集群的（客户端）配置文件的目录
export HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop 
```
 
* cp conf/spark-defaults.conf.template [conf/spark-defaults.conf](https://spark.apache.org/docs/latest/running-on-yarn.html#configuration)
```
# Configure Spark on YARN
spark.master=yarn
spark.submit.deployMode=client
spark.yarn.jars=local:/usr/local/spark/jars/*
```

# spark submit on yarn
```
$ ./bin/spark-submit --class org.apache.spark.examples.SparkPi \
    --master yarn \
    --deploy-mode cluster \
    examples/jars/spark-examples_2.12-3.2.1.jar \
    10 
```