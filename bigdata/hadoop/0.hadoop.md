* https://hadoop.apache.org/
* https://hadoop.apache.org/docs/r1.0.4/cn/index.html
* https://www.w3cschool.cn/hadoop/

# 三大组件
* HDFS - 是一个高可靠、高吞吐量的分布式文件系统
* YARN - (Yet Another Resource Negotiator，另一种资源协调者)，是效率更高的资源管理系统
* MapReduce - 是 Hadoop 核心计算框架，适用于大规模数据集(大于1TB)并行运算的编程模型，包括 Map(映射)和 Reduce(规约) 两部分。
当启动一个 MapReduce 任务时，Map 端会读取 HDFS 上的数据，将数据映射成所需要的键值对类型并传到 Reduce 端。Reduce 端接收 Map 端传过来的键值对类型的数据，根据不同键进行分组，对每一组键相同的数据进行处理，得到新的键值对并输出到 HDFS，这就是 MapReduce 的核心思想。

# 安装依赖
```
yum install -y java-1.8.0-openjdk java-1.8.0-openjdk-devel
```
```
#which java 找到java路径
echo "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk" >>/etc/profile
source /etc/profile
```

* 安装zookeeper
……

# 下载hadoop
```
wget https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.3.2.tar.gz
tar -xzf hadoop-3.3.2.tar.gz
ln -s hadoop-3.3.2 hadoop
```

* vim etc/hadoop/hadoop-env.sh
```
# 设置为 Java 安装的根目录
JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk

# 修改日志目录
export HADOOP_LOG_DIR=/data/hadoop/logs

# 修改pid文件存储路径
export HADOOP_PID_DIR=/data/hadoop/pids
```

# 系统配置
* 创建用户目录
```
useradd  hadoop

mkdir -p /data/hadoop/{data,logs,pids}
chown -R hadoop.hadoop /data/hadoop
chown -R hadoop.hadoop /usr/local/hadoop/
```
* 赋予Hadoop用户sodu权限
```
sed -i '$ahadoop  ALL=(ALL)  NOPASSWD: NOPASSWD: ALL' /etc/sudoers
```
* 配置ssh免密登陆
```
su - hadoop
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 0600 ~/.ssh/authorized_keys
ssh localhost
ssh hadoop01
ssh hadoop02
ssh hadoop03
```