
* https://hadoop.apache.org/docs/r3.2.3/hadoop-project-dist/hadoop-common/ClusterSetup.html
* https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html
* https://cloud.tencent.com/developer/article/1501886
* https://developer.aliyun.com/article/606740

# HDFS 高可用的实现原理
### Active/Standby NameNode
* 在典型的 HA 集群中，两台或多台独立的机器被配置为 NameNode。在任何时间点，只有一个 NameNode 处于Active状态，而其他 NameNode 处于Standby状态。，否则，命名空间状态将很快在两者之间产生分歧，从而冒着数据丢失或其他不正确结果的风险。为了确保这个属性并防止所谓的“脑裂场景”，JournalNodes 将永远只允许一个 NameNode 一次成为写入者。
* 为了让备用节点(Standby NameNode)保持与活动节点(Active NameNode)的状态同步，两个节点都与一组名为“JournalNodes”（JN）的独立守护进程通信。当Active NameNode执行任何命名空间修改时，它会将修改记录持久地记录到大多数的 JN 中。Standby NameNode能够从 JN 中读取编辑，并不断地观察它们以了解对编辑日志的更改。当Standby NameNode看到编辑时，它将它们应用到自己的命名空间
* 为了提供快速故障转移，Standby NameNode还必须具有有关集群中块位置的最新信息。为了实现这一点，DataNode 配置了所有 NameNode 的位置，并向所有 NameNode 发送块位置信息和心跳。

* JournalNode： 运行 JournalNode 的机器。JournalNode 守护进程相对轻量级，因此这些守护进程可以合理地与其他 Hadoop 守护进程（例如 NameNodes、JobTracker 或 YARN ResourceManager）并置在机器上。注意：必须至少有 3 个 JournalNode 守护进程，因为Editlog修改必须写入大多数 JN。这将允许系统容忍单台机器的故障。您也可以运行 3 个以上的 JournalNode，但为了实际增加系统可以容忍的故障数量，您应该运行奇数个 JN（即 3、5、7 等）。请注意，当使用 N 个 JournalNode 运行时，系统最多可以容忍 (N - 1) / 2 次故障并继续正常运行。

### Qurom Journal Manager(QJM) 共享存储
>Active NameNode 和 Standby NameNode 之间共享 一个 EditLog 文件
1. Active NameNode 会定期地把 修改命名空间或删除备份数据块等操作 记录到 EditLog，同时写到 JournalNode 的 多数节点 中。
2. Standby NameNode 会一直监听 JournalNode 上 EditLog 的变化，如果 EditLog 有改动，Standby NameNode 就会读取 EditLog 并与当前的命名空间合并。
3. Active NameNode 出现故障时，Standby NameNode 会保证已经从 JN 上读取了所有 EditLog 并与命名空间合并，然后才会从 Standby 切换为 Active。


# 主机列表
hadoop01 | hadoop02 | hadoop03 
---|---|---
JournalNode | JournalNode | JournalNode
NameNode | NameNode | NameNode
DataNode | DataNode | DataNode
zookeeper | zookeeper | zookeeper
ZKFC | ZKFC | ZKFC

# 配置/etc/hosts
>也可以使用内部DNS域名
```
10.10.62.120  hadoop01
10.10.90.244  hadoop02
10.10.126.148 hadoop03
```
```
hostnamectl --static set-hostname hadoop01
hostnamectl --static set-hostname hadoop02
hostnamectl --static set-hostname hadoop03
```


### 自动故障转移
> 默认情况下，即使Active NameNode发生故障，系统也不会自动触发从Active NameNode 到Standby NameNode 的故障转移
* 自动故障转移向 HDFS 部署添加了两个新组件：ZooKeeper quorum 和 ZKFailoverController 进程（缩写为 ZKFC)
* ZooKeeper quorum
  * 故障检测——集群中的每台 NameNode 机器都在 ZooKeeper 中维护一个持久会话。如果机器崩溃，ZooKeeper 会话将过期，通知其他 NameNode 应该触发故障转移。
  * Active NameNode 选举 - ZooKeeper 提供了一种简单的机制来专门将节点选为Active NameNode。如果当前Active NameNode 崩溃，另一个节点可能会在 ZooKeeper 中获取特殊的排他锁，指示它应该成为下一个Active NameNode
* ZKFailoverController (ZKFC)
  * 健康监控 - ZKFC 使用健康检查命令定期 ping 其本地 NameNode。只要 NameNode 及时响应健康状态，ZKFC 就认为该节点是健康的。如果节点崩溃、假死或以其他方式进入不健康状态，健康监视器会将其标记为不健康。
  * ZooKeeper 会话管理 - 当本地 NameNode 健康时，ZKFC 在 ZooKeeper 中保持一个打开的会话。如果本地 NameNode 处于活动状态，它还拥有一个特殊的znode“锁”。此锁使用 ZooKeeper 对“临时”节点的支持；如果会话过期，锁节点将被自动删除。
  * 基于 ZooKeeper 的选举 —— 如果本地 NameNode 是健康的，并且 ZKFC 看到当前没有其他节点持有znode锁 ，它会自己尝试获取锁。如果成功，那么它“赢得了选举”，并负责运行故障转移以使其本地 NameNode 处于活动状态。故障转移过程类似于上面描述的手动故障转移：首先，如果需要，先前的活动被隔离，然后本地 NameNode 转换为活动状态。


# 配置NameNode
* etc/hadoop/core-site.xml:
```xml
<configuration>
    <!-- 指定hdfs的nameservice的监听地址 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://data-test</value>
    </property>
    <!-- 指定HDFS数据存放路径 -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/data/hadoop/data</value>
    </property>
    <!-- 为自动故障转移设置zookeeper集群 -->
    <property> 
        <name>ha.zookeeper.quorum</name> 
        <value>hadoop01:2181,hadoop02:2181,hadoop03:2181</value> 
    </property>
</configuration>
```

* etc/hadoop/hdfs-site.xml (配置这个nameservice中有几个namenode）
```xml
<configuration>
    <!--指定hdfs的nameservice名称为data-test，需要和core-site.xml中的保持一致 -->
    <property>
        <name>dfs.nameservices</name>
        <value>data-test</value>
    </property>
    <!-- 配置nameservices中NameNode ID列表 -->
    <property>
        <name>dfs.ha.namenodes.data-test</name>
        <value>nn1,nn2,nn3</value>
    </property>
    <!-- 配置nameservices中每个NameNode的RPC监听地址 -->
    <property>
        <name>dfs.namenode.rpc-address.data-test.nn1</name>
        <value>hadoop01:8020</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.data-test.nn2</name>
        <value>hadoop02:8020</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.data-test.nn3</name>
        <value>hadoop03:8020</value>
    </property>
    <!-- 配置nameservices中每个NameNode的HTTP监听地址 -->
    <property>
        <name>dfs.namenode.http-address.data-test.nn1</name>
         <value>hadoop01:9870</value>
    </property>
    <property>
         <name>dfs.namenode.http-address.data-test.nn2</name>
         <value>hadoop02:9870</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.data-test.nn3</name>
        <value>hadoop03:9870</value>
    </property>
    <!-- 配置NameNode元数据 在JournalNode 上的存储地址 -->
    <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://hadoop01:8485;hadoop02:8485;hadoop03:8485/data-test</value>
    </property>
    <!-- 访问代理类：HDFS客户端确定哪个NameNode为Active -->
    <property>
        <name>dfs.client.failover.proxy.provider.data-test</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    <!-- 指定JournalNode在本地磁盘存放数据的位置 -->
    <property> 
        <name>dfs.journalnode.edits.dir</name> 
        <value>${hadoop.tmp.dir}/journal</value> 
    </property>
    <!-- 开启自动故障转移 -->
    <property> 
        <name>dfs.ha.automatic-failover.enabled</name> 
        <value>true</value> 
    </property>
    <!-- 配置隔离机制，同一时该只能有一台服务器对外响应-->
    <property>
        <name>dfs.ha.fencing.methods</name>
        <value>
        sshfence
        shell(/bin/true)
        </value>
    </property>   
    <!-- 使用sshfence隔离机制时需要ssh免登陆 -->
    <property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/home/hadoop/.ssh/id_rsa</value>
    </property>  
    <!-- 配置sshfence隔离机制超时时间 -->
    <property>
        <name>dfs.ha.fencing.ssh.connect-timeout</name>
        <value>30000</value>
    </property>
    <!--关闭hdfs权限检查-->
    <property>
        <name>dfs.permissions</name>
        <value>false</value>
    </property>
</configuration>
```

* etc/hadoop/workers
```
hadoop01
hadoop02
hadoop03
```

* 分发配置到其它节点
```
scp -r etc/hadoop/ hadoop@hadoop02:/usr/local/hadoop/etc/
scp -r etc/hadoop/ hadoop@hadoop03:/usr/local/hadoop/etc/
```

# 启动进程
```
# 在 ZooKeeper 中初始化所需的状态,通过从其中一台 NameNode
bin/hdfs zkfc -formatZK
#启动3个节点zkfc守护进程
bin/hdfs --daemon start zkfc

#启动3个节点journalnode
bin/hdfs --daemon start journalnode   # 等待守护程序在每台相关机器上启动完成

# 启动namenode
[hadoop01 ~]$ bin/hdfs namenode -format
[hadoop01 ~]$ bin/hdfs --daemon start namenode
[hadoop02 ~]$ bin/hdfs namenode -bootstrapStandby   #复制己经格式化过的namenode的元数据目录的内容
[hadoop02 ~]$ bin/hdfs --daemon start namenode
[hadoop03 ~]$ bin/hdfs namenode -bootstrapStandby
[hadoop03 ~]$ bin/hdfs --daemon start namenode

# 启动3个节点datanode
bin/hdfs --daemon start datanode
```

### 一键启动所有组件
>需配置etc/hadoop/workers和ssh免密登陆
```
sbin/start-dfs.sh
sbin/stop-dfs.sh
```


# 手动切换Active（不需要zookeeper）
* [hdfs管理命令](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html#Administrative_commands)
```
# 手动将nn1转换为Active NameNode（为防止脑裂，必须在所有NameNode都启动的状态下才能转换）
bin/hdfs haadmin -transitionToActive nn1
# 查看nn1状态
bin/hdfs haadmin -getServiceState nn1
```

# 测试自动故障转移
* 通过访问 NameNode Web 界面来判断哪个NameNode处于Active状态
* kill -9 <pid of NN > 来模拟 JVM 崩溃
* 在触发您希望测试的中断后，另一个 NameNode 应该会在几秒钟内自动变为活动状态。检测故障和触发故障转移所需的时间取决于ha.zookeeper.session-timeout.ms的配置，但默认为 5 秒。



# 负载均衡器代理一组NameNode
* 如果您在负载均衡器（例如Azure或AWS ）后面运行一组 NameNode,并且希望负载均衡器指向Active NameNode
```
#检测主节点 /isActive
http://NN_HOSTNAME:9870/isActive  将返回 200 状态码响应，否则返回 405
```