* https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/topology-spread-constraints/
>你可以使用 拓扑分布约束（Topology Spread Constraints） 来控制 Pod 在集群内故障域之间的分布， 例如区域（Region）、可用区（Zone）、节点和其他用户自定义拓扑域。 这样做有助于实现高可用并提升资源利用率
```yml
kind: Pod
apiVersion: v1
metadata:
  name: mypod
  labels:
    foo: bar
spec:
  topologySpreadConstraints:   
  - maxSkew: 1    #定义目标拓扑中匹配 Pod 的数量与 全局最小值（例如，如果你有 3 个可用区，分别有 2、2 和 1 个匹配的 Pod，则 MaxSkew 设为 1， 且全局最小值为 1） 之间的最大允许差值，如果你选择 whenUnsatisfiable: DoNotSchedule；如果你选择 whenUnsatisfiable: ScheduleAnyway，则该调度器会更为偏向能够降低偏差值的拓扑域
    whenUnsatisfiable: DoNotSchedule  #如果 Pod 不满足分布约束时如何处理，DoNotSchedule（默认）告诉调度器不要调度。ScheduleAnyway 告诉调度器仍然继续调度，只是根据如何能将偏差最小化来对节点进行排序。
    topologyKey: zone # topologyKey为节点label的key,节点label的key为zone ,且values相同的节点被认为在相同拓扑域中（也就是具有相同key:values的实例为一个域）调度器将尝试在每个域中放置均衡数量的pod
    labelSelector:    # 用于查找pod labels。匹配此标签的 Pod 将被统计，以确定相应拓扑域中 Pod 的数量
      matchLabels:
        foo: bar
  containers:
  - name: pause
    image: k8s.gcr.io/pause:3.1
```
* 当 Pod 定义了不止一个 topologySpreadConstraint，这些约束之间是逻辑与的关系,kube-scheduler 会为新的 Pod 寻找一个能够满足所有约束的节点
```yml
kind: Pod
apiVersion: v1
metadata:
  name: mypod
  labels:
    foo: bar
spec:
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        foo: bar
  - maxSkew: 1
    topologyKey: node
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        foo: bar
  containers:
  - name: pause
    image: k8s.gcr.io/pause:3.1
```

# 与节点亲和性和节点选择算符的相互作用
* 如果 Pod 定义了 spec.nodeSelector 或 spec.affinity.nodeAffinity， 调度器将在偏差计算中跳过不匹配的节点。
>假设你有一个跨可用区 A 到 C 的 5 节点集群：而且你知道可用区 C 必须被排除在外。在这种情况下，可以按如下方式编写清单， 以便将 Pod mypod 放置在可用区 B 上，而不是可用区 C 上。 同样，Kubernetes 也会一样处理 spec.nodeSelector。
```yml
kind: Pod
apiVersion: v1
metadata:
  name: mypod
  labels:
    foo: bar
spec:
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        foo: bar
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: zone
            operator: NotIn
            values:
            - zoneC
  containers:
  - name: pause
    image: k8s.gcr.io/pause:3.1
```

# 隐式约定
- 只有与新来的 Pod 具有相同命名空间的 Pod 才能作为匹配候选者。
- 调度器会忽略没有任何 topologySpreadConstraints[*].topologyKey 的节点,这意味着：
  - 位于这些节点上的 Pod 不影响 maxSkew 计算
  - 新的 Pod 没有机会被调度到这类节点上

# 集群级别的默认约束
* 集群默认拓扑分布约束在且仅在以下条件满足时才会被应用到 Pod 上:
  - Pod 没有在其 .spec.topologySpreadConstraints 中定义任何约束。
  - Pod 隶属于某个 Service、ReplicaSet、StatefulSet 或 ReplicationController。
```yml
apiVersion: kubescheduler.config.k8s.io/v1beta3
kind: KubeSchedulerConfiguration

profiles:
  - schedulerName: default-scheduler
    pluginConfig:
      - name: PodTopologySpread
        args:
          defaultConstraints:
            - maxSkew: 1
              topologyKey: topology.kubernetes.io/zone
              whenUnsatisfiable: ScheduleAnyway
          defaultingType: List
```

# 内置默认约束
* 如果你没有为 Pod 拓扑分布配置任何集群级别的默认约束， kube-scheduler 的行为就像你指定了以下默认拓扑约束一样：
```yml
defaultConstraints:
  - maxSkew: 3
    topologyKey: "kubernetes.io/hostname"
    whenUnsatisfiable: ScheduleAnyway
  - maxSkew: 5
    topologyKey: "topology.kubernetes.io/zone"
    whenUnsatisfiable: ScheduleAnyway
```

# 已知局限性
* 当 Pod 被移除时，无法保证约束仍被满足。例如，缩减某 Deployment 的规模时，Pod 的分布可能不再均衡。你可以使用 [Descheduler](https://github.com/kubernetes-sigs/descheduler) 来重新实现 Pod 分布的均衡。
* 具有污点的节点上匹配的 Pod 也会被统计
* 该调度器不会预先知道集群拥有的所有可用区和其他拓扑域。 拓扑域由集群中存在的节点确定。在自动扩缩的集群中，如果一个节点池（或节点组）的节点数量缩减为零， 而用户正期望其扩容时，可能会导致调度出现问题。 因为在这种情况下，调度器不会考虑这些拓扑域，因为其中至少有一个节点。
你可以通过使用感知 Pod 拓扑分布约束并感知整个拓扑域集的集群自动扩缩工具来解决此问题。
